{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b510281-7ebb-4eb0-9703-9fc4910e372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61eec609-5ba3-4e26-acd5-afa39200b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123) # reproducibility\n",
    "\n",
    "\n",
    "file_name = \"../data/songs_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2027f44f-94d3-4ca4-937f-7073426e49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongsDataset(Dataset):\n",
    "    \"\"\"Tokenize data when calling __getitem__\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        with open(file_name, \"r\") as f:\n",
    "            song = \"\"\n",
    "            for line in f:\n",
    "                if line==\"\\n\":\n",
    "                    self.data.append({\"text\":song})\n",
    "                    song = \"\"\n",
    "                else:\n",
    "                    song += line \n",
    "                    song += \" \"\n",
    "                    \n",
    "    #def __getitem__(self, i):\n",
    "    #    inputs = self.tokenizer(self.data[i][\"text\"])\n",
    "    #    return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fda9b7-0e7a-42c8-b6cf-b763f35b186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_songs = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0eabf8e-9a4e-4ae6-be49-2b32b1e96308",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SongsDataset(file_name, tokenizer)\n",
    "# trainset = trainset.map(lambda example: tokenizer(example, padding=True, truncation=True),\n",
    "#     batched=True,\n",
    "#     batch_size=16\n",
    "# )\n",
    "trainset = tokenizer(songs)#, padding=True, truncation=True)\n",
    "train_dataloader = DataLoader(trainset, batch_size=1, collate_fn=lambda x: x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e5e7edb-228f-435a-a8ce-0d50f98b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SongsDataset(file_name, tokenizer).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b46b0-1b10-494c-b96e-85dcd7c57d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model=model,\n",
    "#                   args=training_args,\n",
    "#                   data_collator=data_collator,\n",
    "#                   train_dataset=train_dataset,\n",
    "#                   eval_dataset=eval_dataset,\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22997534-659e-49f5-93cc-59dbe31049e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe3fbab8450>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3a0d633e-6a63-4bad-b1a4-8d0a23919fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83a0012a7146c9bc625c05623865dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=151, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GPT2LMHeadModel object argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16986/3691463823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# batch = ([text1, text2], [0, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: GPT2LMHeadModel object argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = 3 * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    for batch_i, batch in enumerate(train_dataloader):\n",
    "        print(batch)\n",
    "        # batch = ([text1, text2], [0, 1])\n",
    "\n",
    "        output = model(**batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    for batch_i, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        loss += output.loss\n",
    "    \n",
    "    avg_val_loss = loss / len(eval_dataloader)\n",
    "    print(f\"Validation loss: {avg_val_loss}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\"Saving checkpoint!\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(f\"best_val_loss = {best_val_loss}\")\n",
    "        #torch.save({\n",
    "        #    'epoch': epoch,\n",
    "        #    'model_state_dict': model.state_dict(),\n",
    "        #    'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #    'val_loss': best_val_loss,\n",
    "        #    },\n",
    "        #    f\"checkpoints/epoch_{epoch}.pt\"\n",
    "        #)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8192915-b86e-4e5d-84e0-9dfb36f703f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d2a8b4f-95e9-4f32-b253-df6c35055cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = []\n",
    "with open(file_name, \"r\") as f:\n",
    "    song = \"\"\n",
    "    for line in f:\n",
    "        if line==\"\\n\":\n",
    "            songs.append(song)\n",
    "            song = \"\"\n",
    "        else:\n",
    "            song += line \n",
    "            song += \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a893d9f4-8e0f-4fbd-9f05-b8661caeef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2fb25148-c9e8-4bd9-965a-557b8795b8e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "training mode is expected to be boolean",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16986/2570938561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \"\"\"\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: training mode is expected to be boolean"
     ]
    }
   ],
   "source": [
    "model.train(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccefbf8c-404a-4c90-8881-557009dd7cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = AutoModelWithLMHead.from_pretrained('flax-community/papuGaPT2')\n",
    "len(tokenizer(songs[0]).input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c34dca8d-69ff-460d-8476-152f5119a0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(songs[0]).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a2b3eaf-4034-49be-9526-6fbdb271bb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Poprzez morza i przez oceany\\n płynie statek wśród wspienionych fal\\n a na statku chłopak zakochany \\n zakochany w  morzu tyle lat \\n Biała mewo, leć daleko stąd,\\n leć daleko na ojczyzny ląd,\\n poleć, powiedz, że marynarz chwat,\\xa0\\n morze kocha od dziecinnych lat\\n  Nad okretem biała mewa leci \\n nad podkładem gdzies się zerwał wiatr \\n a nad statkiem czarne chmury wiszą\\n a marynarz swą melodię gra \\n  Na ojczystej ziemi pioska padła \\n biała mewa też upadła w piach \\n i o marynarzu opowiada \\n co na morzy przezył tyle lat \\n polec ...\\n '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bae977d-b79d-4732-820e-e7c840e1d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('flax-community/papuGaPT2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2')\n",
    "generator = pipeline('text-generation', model='flax-community/papuGaPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4796b8-e2d1-491b-a485-fbe6276aa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='flax-community/papuGaPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa40afb-2ef8-4f5d-a8f5-9826694fb832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Najwiekszym polskim poeta byl Julian Przyboczny, najdluzej poetkom znany w zaborach, a takze ich przyjaciele, ale tez i wielu, ktorzy zyja i pamietaja je z kart jego poezji. W tych czasach jednak'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Najwiekszym polskim poeta byl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622bc7d1-29a6-4081-abdd-1cd459855927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_pre_hooks', '_state_dict_hooks', '_load_state_dict_pre_hooks', '_modules', 'config', 'name_or_path', 'model_parallel', 'device_map'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96260256-4600-4b1c-8834-2a8651738cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_list = [func for func in dir(model) if callable(getattr(model, func)) and not func.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a3eed-cc57-4334-9921-4f8178a3c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab3fb5-b455-4fac-9a72-853b489c6fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
