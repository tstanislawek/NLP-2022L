{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc30bdd-b89f-49eb-850f-5da716ff1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66310ecc-0d89-41dc-998d-9f63c6d20cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bf8921-39dc-4814-b6ea-ebf9ce5f4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec545328-2008-4d1d-a305-ef009e9c3620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1ef25d-a025-4931-8936-8e64b60c94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - validation - test split\n",
    "#data = pd.read_csv(\"data/songs_data_tokenized2.csv\", encoding='utf-8')\n",
    "#N = len(data)\n",
    "#X = np.array(list(range(N)))\n",
    "#np.random.shuffle(X)\n",
    "\n",
    "#train = data.loc[X[:int(.8*N)]]\n",
    "#validation = data.loc[X[int(.8*N):int(.9*N)]]\n",
    "#test = data.loc[X[int(.9*N):]]\n",
    "\n",
    "#train.to_csv(\"data/train.csv\", index=False)\n",
    "#validation.to_csv(\"data/validation.csv\", index=False)\n",
    "#test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\", encoding='utf-8')\n",
    "validation = pd.read_csv(\"data/validation.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e138d89-74b9-4708-bb0b-dde039b09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"124M\"\n",
    "checkpoint_dir = 'checkpoint'\n",
    "file_name = \"data/train.csv\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_dir = \"models/gpt2_validation\"\n",
    "bucket_name = \"poem-generator-checkpoints\"\n",
    "run_name='disco_polo_gen_gpt2_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21ffffb-1527-4a56-8a09-65cac5d51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(model_dir, model_name)):\n",
    "    gpt2.download_gpt2(model_dir=model_dir, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ec19a2-e96e-4be2-8c30-19cd249cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(sess,\n",
    "             run_name='run1',\n",
    "             checkpoint_dir='checkpoint',\n",
    "             model_name=None,\n",
    "             model_dir='models',\n",
    "             prefix=\"<|endoftext|>\",\n",
    "             all=False):\n",
    "\n",
    "    import tarfile\n",
    "    import os\n",
    "    import json\n",
    "    import requests\n",
    "    import sys\n",
    "    import shutil\n",
    "    import re\n",
    "    from tqdm import tqdm, trange\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    from tensorflow.python.client import device_lib\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import csv\n",
    "    import argparse\n",
    "    from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
    "    from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
    "    from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
    "    \n",
    "    batch_size=1\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    enc = encoder.get_encoder(checkpoint_path)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if prefix:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        context_tokens = enc.encode(prefix)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model.model(hparams=hparams, X=tokens,\n",
    "                                past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(model.past_shape(\n",
    "            hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    output = step(hparams, context)\n",
    "\n",
    "    out = sess.run(output, feed_dict={\n",
    "                    context: batch_size * [context_tokens]\n",
    "                })\n",
    "\n",
    "    if all:\n",
    "        return out['logits'][0, :, :]  # all logits starting from the second token, n logits for n tokens\n",
    "    return out['logits'][0, -1, :]  # logits for next token\n",
    "\n",
    "\n",
    "def get_perplexity(sess,\n",
    "               run_name='run1',\n",
    "               checkpoint_dir='checkpoint',\n",
    "               model_name=None,\n",
    "               model_dir='models',\n",
    "               prefix=\"<|endoftext|>\",\n",
    "               continuation=\"Hello\"):\n",
    "    \n",
    "    import tarfile\n",
    "    import os\n",
    "    import json\n",
    "    import requests\n",
    "    import sys\n",
    "    import shutil\n",
    "    import re\n",
    "    from tqdm import tqdm, trange\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    from tensorflow.python.client import device_lib\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import csv\n",
    "    import argparse\n",
    "    from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
    "    from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
    "    from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns perplexity score for given continuation of a given prefix.\n",
    "    \n",
    "    Examples:\n",
    "    perplexity(sess, model_name=\"124M\", prefix=\"Hello, my name is\", continuation=\" James Smith, I am an engineer\")  # returns 17.3124\n",
    "    perplexity(sess, model_name=\"124M\", prefix=\"Hello, my name is\", continuation=\" very else whatever general cat meow.\")  # returns 5197.99\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size=1\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    enc = encoder.get_encoder(checkpoint_path)\n",
    "\n",
    "    context_tokens = enc.encode(prefix)\n",
    "    \n",
    "    context_size = len(context_tokens)\n",
    "    continuation_tokens = enc.encode(continuation)\n",
    "    # my line\n",
    "    cutting_point = min(len(continuation_tokens), 1024-context_size-1)\n",
    "    continuation_tokens = continuation_tokens[:cutting_point]\n",
    "    \n",
    "    full_sentence = prefix+continuation\n",
    "\n",
    "    logits = get_logits(sess, run_name, checkpoint_dir, model_name, model_dir, full_sentence, all=True)\n",
    "\n",
    "    logits = logits[context_size-1:-1, :]  # only continuation logits\n",
    "    logitmeans = np.mean(logits, axis=1)\n",
    "    logits = logits - logitmeans[:, None]\n",
    "    explogits = np.exp(logits)\n",
    "    probabs = explogits / np.sum(explogits,axis=1)[:, None]\n",
    "    \n",
    "    probab_scores = np.nan_to_num([probabs[i, index] for i, index in enumerate(continuation_tokens)])\n",
    "    perplexity = 2 ** (-np.mean(np.log2(probab_scores)))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def generate_song_part(context, truncation, common_params_dict, savepath=None, min_length_stanza_chars=100,\n",
    "                       min_length_song_chars=500, chorus_needed=False):\n",
    "    \n",
    "    res = gpt2.generate(sess,\n",
    "              prefix=context,\n",
    "              truncate=truncation,\n",
    "              **common_params_dict\n",
    "              )\n",
    "    \n",
    "    good_results = []\n",
    "    for st in res:\n",
    "        print(f\"Length of generated chunk (chars) = {len(st) - len(context)}, (context len = {len(context)})\")\n",
    "        if len(st) - len(context) > min_length_stanza_chars:# and st.count(\"<RBEG>\") == 0:\n",
    "            st = st.replace(\"<RBEG>\", \"\").replace(\"<EOST>\", \"\")\n",
    "            good_results.append(st)\n",
    "            print(st)\n",
    "    print(f\"Out of {len(res)} initial stanzas, {len(good_results)} were left.\")\n",
    "          \n",
    "    save_count = 0\n",
    "    to_continue = []\n",
    "    if savepath:\n",
    "        for song in good_results:\n",
    "            if len(song) > min_length_song_chars:\n",
    "                if savepath == \"return\":\n",
    "                    return song\n",
    "                else:\n",
    "                    song_name = \"sample_\" + str(random.randrange(1_000_000_000))\n",
    "                    with open(savepath+song_name, \"w+\") as file:\n",
    "                        file.write(song)\n",
    "                        save_count += 1\n",
    "            else:\n",
    "                to_continue.append(song)\n",
    "        print(f\"{save_count} songs were saved.\")\n",
    "               \n",
    "            \n",
    "    for song in to_continue:\n",
    "        if chorus_needed:\n",
    "            context = song + \"<RBEG>\"\n",
    "            truncation = \"<EOST>\"\n",
    "            chorus_needed = False\n",
    "        else:\n",
    "            context = song\n",
    "            truncation = \"<EOST>\"\n",
    "        \n",
    "        generate_song_part(context=context, truncation=truncation, common_params_dict=common_params_dict,\n",
    "                           savepath=\"./samples/\", min_length_stanza_chars=min_length_stanza_chars,\n",
    "                           min_length_song_chars=min_length_song_chars, chorus_needed=chorus_needed)\n",
    "    \n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, name, mode, encoding=\"utf-8\"):\n",
    "        self.file = open(name, mode, encoding=encoding)\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "    def __del__(self):\n",
    "        sys.stdout = self.stdout\n",
    "        self.file.close()\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        try:\n",
    "            self.stdout.write(data)\n",
    "        except UnicodeEncodeError as err:\n",
    "            self.stdout.write(f\"Writing log didn't succeed due to {err}.\")\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "\n",
    "gpt2.get_perplexity = get_perplexity\n",
    "gpt2.get_logits = get_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8964183-5b4b-4da7-9fb5-b9435d55d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2.get_perplexity(sess,\n",
    "#                run_name=run_name,\n",
    "#                checkpoint_dir='checkpoint',\n",
    "#                model_name=\"124M\",\n",
    "#                model_dir=model_dir,\n",
    "#                prefix=prefix,\n",
    "#                continuation=\"generated\")\n",
    "\n",
    "\n",
    "\n",
    "common_params_dict = {\"run_name\":run_name, \"length\":100, \"temperature\":0.9,\n",
    "                      \"top_p\":0.95, \"return_as_list\":True, \"nsamples\":1, \"batch_size\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4d5b84-877a-42cd-aaa6-0f74320085bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': 'disco_polo_gen_gpt2_validation',\n",
       " 'length': 100,\n",
       " 'temperature': 0.9,\n",
       " 'top_p': 0.95,\n",
       " 'return_as_list': True,\n",
       " 'nsamples': 1,\n",
       " 'batch_size': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8865802d-e7a1-41b7-99df-d52fb7f3d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "Loading dataset...\n",
      "dataset has 3092002 tokens\n",
      "Training...\n",
      "Saving checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.data-00000-of-00001\n",
      "INFO:tensorflow:497800\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.meta\n",
      "INFO:tensorflow:500900\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.index\n",
      "INFO:tensorflow:500900\n",
      "======== SAMPLE 1 ========\n",
      " si noc i za pamiętam\n",
      "Dziś dłoń i jesteś możesz przy Tobie będzie mnie\n",
      "\n",
      "2.\tMiałem ciepło\n",
      "Szalone jest ten ładne\n",
      "Gdy jesteś mam takie, a Ty ten błękit nam\n",
      "\n",
      "<RBEG>Ref.:\n",
      "Dobrze wiem, dziś słodki jest\n",
      "Wyć z Tobą, że mnie przy Tobie dziś\n",
      "Znaczyłem się noc i za pamiętam\n",
      "Dziś dłoń i jesteś możesz przy Tobie będzie mnie\n",
      "\n",
      "<RBEG>Ref.:\n",
      "Dobrze wiem, dziś słodki jest\n",
      "Wyć z Tobą, że mnie przy Tobie dziś\n",
      "Znaczyłem się noc i za pamiętam\n",
      "Dziś dłoń i jesteś możesz przy Tobie będzie mnie \n",
      "\n",
      "<RBEG>Ref.:\n",
      "Dobrze wiem, dziś słodki jest\n",
      "Wyć z Tobą, że mnie przy Tobie dziś\n",
      "Znaczyłem się noc i za pamiętam\n",
      "Dziś dłoń i jesteś możesz przy Tobie będzie mnie, (4 <REND><REND>x<EOST>)<|endoftext|>\n",
      "<|startoftext|>Nie mogłem, że to co, więc bzy?\n",
      "Nie wiedziałem, że to co? Ty mnie wciąż czerech ci w snach trwać\n",
      "Wciąż częściem, że chcę wiedziałem już\n",
      "Nie mogłem, że to co, więc bzy.\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Nie kochać będę, tylko kochać, nie chcesz mi powie\n",
      "A zapomnisz słodkich blask, więc bzy?\n",
      "Nie mogłem na że ja głupie podróż świat\n",
      "Także mi źle przy Tobie powie dziś\n",
      "Nie mogłem na że ja zmienie dziś róż\n",
      "Nie młoda dwa, nie rób nie wiesz\n",
      "Podrób świat, że wszystkie zmieniem ści<REND>\n",
      "<EOST>\n",
      "Zapomniałe raz i po podłudem będzie i łzy\n",
      "Mieszy chcesz mi w słowa pamiętać mieszkie ją łez\n",
      "Że już zmieniłem, że to co: więc bzy?\n",
      "Oooo...\n",
      "<EOST>\n",
      "Nie musiałem, że jest jak ja chciałem\n",
      "Tylko tak się dziś powiem jak zmienia wycyłem\n",
      "Ty to jest mieć że to co, więc dziś dziś znów<EOST>.<|endoftext|>\n",
      "<|startoftext|>1. To nie kwiat wciąż uśmiechasz mój\n",
      "To kochasz się na mamy okończyłeś\n",
      "Czuję się na to się kochałeś\n",
      "Ja nie nie nie nie nie mówić Cię\n",
      "Ja nie nie nie nie nie mówić Cię\n",
      "<EOST>\n",
      "<RBEG>Ref: Jak ja ja ja ja ja ja ja ja ja ja ja\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.stdout = Tee(\"log_data2.dat\", mode=\"a\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b8f098-1193-4aa7-bc4d-c175136ba5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Testing saving output...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e50eaa-4092-42b6-87ee-844b1029f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 06:49:56.776548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-26 06:49:56.860855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-26 06:49:56.860907: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-26 06:49:56.860933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pytorch-1-10-20220326-144724): /proc/driver/nvidia/version does not exist\n",
      "2022-05-26 06:50:08.449582: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for song in validation.song:\n",
    "#        splitted = song.split(\"\\n\")\n",
    "        #print(splitted)\n",
    "#        context = splitted[0]\n",
    "#        del splitted[0]\n",
    "#        continuation = \"\".join(splitted)\n",
    "        #print(song)\n",
    "        #print(continuation)\n",
    "        #print(context)\n",
    "sess = gpt2.start_tf_sess() \n",
    "\n",
    "gpt2.finetune(sess,\n",
    "                  model_dir=model_dir,\n",
    "                  dataset=file_name,\n",
    "                  model_name=model_name,\n",
    "                  reuse=False,\n",
    "                  steps=8000,\n",
    "                  run_name=run_name,\n",
    "                  print_every=10,\n",
    "                  sample_every=200,\n",
    "                  save_every=500, \n",
    "                  only_train_transformer_layers=False,\n",
    "                  accumulate_gradients = 1,\n",
    "                  overwrite=True,\n",
    "                  restore_from=\"latest\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c5d72a-0e10-45f4-9fab-434a0c319ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 18:51:51.040688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-25 18:51:51.114893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-25 18:51:51.114971: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-25 18:51:51.115003: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pytorch-1-10-20220326-144724): /proc/driver/nvidia/version does not exist\n",
      "2022-05-25 18:52:02.780146: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-05-25 18:52:04.525451: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.158562: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.336886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.652409: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.874948: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "2022-05-25 18:54:32.017068: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cast_op.cc:109 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n",
      "2022-05-25 18:54:32.123658: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cwise_ops_common.cc:81 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradients/model/h10/attn/Max_grad/Cast' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n      restore_from=\"latest\"\n    File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n      opt_grads = tf.gradients(ys=loss, xs=train_vars)\nNode: 'gradients/model/h10/attn/Max_grad/Cast'\nOOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\nOriginal stack trace for 'gradients/model/h10/attn/Max_grad/Cast':\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n    opt_grads = tf.gradients(ys=loss, xs=train_vars)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 314, in gradients_v2\n    unconnected_gradients)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 328, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 241, in _MaxGrad\n    return _MinOrMaxGrad(op, grad)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 231, in _MinOrMaxGrad\n    indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1002, in cast\n    x = gen_math_ops.cast(x, base_type, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2015, in cast\n    \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3762, in _create_op_internal\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\n...which was originally created as op 'model/h10/attn/Max', defined at:\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus, reuse=reuse)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 203, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 139, in attn\n    a = multihead_attn(q, k, v)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 127, in multihead_attn\n    w = softmax(w)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 51, in softmax\n    x = x - tf.reduce_max(input_tensor=x, axis=axis, keepdims=True)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3105, in reduce_max\n    _ReductionDims(input_tensor, axis))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3116, in reduce_max_with_dims\n    gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 6114, in _max\n    name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2597/462499367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0maccumulate_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                   \u001b[0mrestore_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                   )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 (_, v_loss, v_summary) = sess.run(\n\u001b[1;32m    344\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mopt_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     feed_dict={context: sample_batch()})\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0msummary_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1396\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradients/model/h10/attn/Max_grad/Cast' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n      restore_from=\"latest\"\n    File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n      opt_grads = tf.gradients(ys=loss, xs=train_vars)\nNode: 'gradients/model/h10/attn/Max_grad/Cast'\nOOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\nOriginal stack trace for 'gradients/model/h10/attn/Max_grad/Cast':\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n    opt_grads = tf.gradients(ys=loss, xs=train_vars)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 314, in gradients_v2\n    unconnected_gradients)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 328, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 241, in _MaxGrad\n    return _MinOrMaxGrad(op, grad)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 231, in _MinOrMaxGrad\n    indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1002, in cast\n    x = gen_math_ops.cast(x, base_type, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2015, in cast\n    \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3762, in _create_op_internal\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\n...which was originally created as op 'model/h10/attn/Max', defined at:\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus, reuse=reuse)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 203, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 139, in attn\n    a = multihead_attn(q, k, v)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 127, in multihead_attn\n    w = softmax(w)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 51, in softmax\n    x = x - tf.reduce_max(input_tensor=x, axis=axis, keepdims=True)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3105, in reduce_max\n    _ReductionDims(input_tensor, axis))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3116, in reduce_max_with_dims\n    gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 6114, in _max\n    name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess() \n",
    "steps = 200\n",
    "scores = []\n",
    "n_steps = []\n",
    "i = 0\n",
    "\n",
    "while i < 3 or (scores[-1] + scores[-2])/2 < scores[-3]:\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    # run model for 200 steps\n",
    "    gpt2.finetune(sess,\n",
    "                  model_dir=model_dir,\n",
    "                  dataset=file_name,\n",
    "                  model_name=model_name,\n",
    "                  reuse=False,\n",
    "                  steps=steps,\n",
    "                  run_name=run_name,\n",
    "                  print_every=10,\n",
    "                  sample_every=200,\n",
    "                  save_every=500, \n",
    "                  only_train_transformer_layers=False,\n",
    "                  accumulate_gradients = 1,\n",
    "                  overwrite=True,\n",
    "                  restore_from=\"latest\"\n",
    "                  )\n",
    "    \n",
    "    # calculate perplexity on test set\n",
    "    print(\"Calculating perplexity...\")\n",
    "    tmp_perplexity = []\n",
    "    ids = np.random.choice(range(len(validation.song)), size=100, replace=False)\n",
    "    sampled_100_songs = validation.song.loc[ids]\n",
    "    for song in tqdm(sampled_100_songs):\n",
    "        splitted = song.split(\"\\n\")\n",
    "        context = splitted[0]\n",
    "        first_eost = song.find(\"<EOST>\")\n",
    "        if first_eost < len(context):\n",
    "            first_eost = min(2 * len(context), len(song))\n",
    "            \n",
    "        continuation = song[len(context):first_eost]\n",
    "        \n",
    "        #sample = generate_song_part(context=context, truncation=\"<RBEG>\", common_params_dict=common_params_dict,\n",
    "        #           savepath=\"return\", min_length_stanza_chars=40, min_length_song_chars=700,\n",
    "        #           chorus_needed=True)\n",
    "        #print(\"Calculating perplexity...\")\n",
    "        perp_i = 0\n",
    "        try:\n",
    "            perp_i = gpt2.get_perplexity(sess,\n",
    "                       run_name=run_name,\n",
    "                       checkpoint_dir='checkpoint',\n",
    "                       model_name=\"124M\",\n",
    "                       model_dir=model_dir,\n",
    "                       prefix=context,\n",
    "                       continuation=continuation)\n",
    "        except  Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        tmp_perplexity.append(perp_i)\n",
    "        \n",
    "    perp = sum(tmp_perplexity) / len(validation)\n",
    "    scores.append(perp)\n",
    "    print(f\"Current perplexity = {perp}\")\n",
    "    # update iterator for stop condition\n",
    "    i += 1\n",
    "    \n",
    "print(\"Training done.\")\n",
    "print(f\"We did {(i+1) * 200} steps in total.\")\n",
    "print(\"Perplexity scores:\")\n",
    "print(scores)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./validation_info.txt\", \"w+\") as f:\n",
    "    f.write(f\"We did {(i+1) * 200} steps in total.\")\n",
    "    f.write(\"Perplexity scores:\")\n",
    "    f.write(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9cc4f-634f-42a3-9dc0-a077d1c628fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e46fec-ea15-4d66-a7ea-37ef01ea7b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3760be-c41b-4ab8-9986-a74efec9731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4327aa-ad91-456f-9e5a-a7f4e15702f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
