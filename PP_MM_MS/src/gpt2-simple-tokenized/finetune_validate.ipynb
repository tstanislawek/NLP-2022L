{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc30bdd-b89f-49eb-850f-5da716ff1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66310ecc-0d89-41dc-998d-9f63c6d20cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bf8921-39dc-4814-b6ea-ebf9ce5f4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec545328-2008-4d1d-a305-ef009e9c3620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1ef25d-a025-4931-8936-8e64b60c94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - validation - test split\n",
    "#data = pd.read_csv(\"data/songs_data_tokenized2.csv\", encoding='utf-8')\n",
    "#N = len(data)\n",
    "#X = np.array(list(range(N)))\n",
    "#np.random.shuffle(X)\n",
    "\n",
    "#train = data.loc[X[:int(.8*N)]]\n",
    "#validation = data.loc[X[int(.8*N):int(.9*N)]]\n",
    "#test = data.loc[X[int(.9*N):]]\n",
    "\n",
    "#train.to_csv(\"data/train.csv\", index=False)\n",
    "#validation.to_csv(\"data/validation.csv\", index=False)\n",
    "#test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\", encoding='utf-8')\n",
    "validation = pd.read_csv(\"data/validation.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e138d89-74b9-4708-bb0b-dde039b09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"124M\"\n",
    "checkpoint_dir = 'checkpoint'\n",
    "file_name = \"data/train.csv\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_dir = \"models/gpt2_validation\"\n",
    "bucket_name = \"poem-generator-checkpoints\"\n",
    "run_name='disco_polo_gen_gpt2_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21ffffb-1527-4a56-8a09-65cac5d51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(model_dir, model_name)):\n",
    "    gpt2.download_gpt2(model_dir=model_dir, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ec19a2-e96e-4be2-8c30-19cd249cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(sess,\n",
    "             run_name='run1',\n",
    "             checkpoint_dir='checkpoint',\n",
    "             model_name=None,\n",
    "             model_dir='models',\n",
    "             prefix=\"<|endoftext|>\",\n",
    "             all=False):\n",
    "\n",
    "    import tarfile\n",
    "    import os\n",
    "    import json\n",
    "    import requests\n",
    "    import sys\n",
    "    import shutil\n",
    "    import re\n",
    "    from tqdm import tqdm, trange\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    from tensorflow.python.client import device_lib\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import csv\n",
    "    import argparse\n",
    "    from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
    "    from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
    "    from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
    "    \n",
    "    batch_size=1\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    enc = encoder.get_encoder(checkpoint_path)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if prefix:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        context_tokens = enc.encode(prefix)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model.model(hparams=hparams, X=tokens,\n",
    "                                past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(model.past_shape(\n",
    "            hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    output = step(hparams, context)\n",
    "\n",
    "    out = sess.run(output, feed_dict={\n",
    "                    context: batch_size * [context_tokens]\n",
    "                })\n",
    "\n",
    "    if all:\n",
    "        return out['logits'][0, :, :]  # all logits starting from the second token, n logits for n tokens\n",
    "    return out['logits'][0, -1, :]  # logits for next token\n",
    "\n",
    "\n",
    "def get_perplexity(sess,\n",
    "               run_name='run1',\n",
    "               checkpoint_dir='checkpoint',\n",
    "               model_name=None,\n",
    "               model_dir='models',\n",
    "               prefix=\"<|endoftext|>\",\n",
    "               continuation=\"Hello\"):\n",
    "    \n",
    "    import tarfile\n",
    "    import os\n",
    "    import json\n",
    "    import requests\n",
    "    import sys\n",
    "    import shutil\n",
    "    import re\n",
    "    from tqdm import tqdm, trange\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    from tensorflow.python.client import device_lib\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import csv\n",
    "    import argparse\n",
    "    from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
    "    from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
    "    from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns perplexity score for given continuation of a given prefix.\n",
    "    \n",
    "    Examples:\n",
    "    perplexity(sess, model_name=\"124M\", prefix=\"Hello, my name is\", continuation=\" James Smith, I am an engineer\")  # returns 17.3124\n",
    "    perplexity(sess, model_name=\"124M\", prefix=\"Hello, my name is\", continuation=\" very else whatever general cat meow.\")  # returns 5197.99\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size=1\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    enc = encoder.get_encoder(checkpoint_path)\n",
    "\n",
    "    context_tokens = enc.encode(prefix)\n",
    "    \n",
    "    context_size = len(context_tokens)\n",
    "    continuation_tokens = enc.encode(continuation)\n",
    "    # my line\n",
    "    cutting_point = min(len(continuation_tokens), 1024-context_size-1)\n",
    "    continuation_tokens = continuation_tokens[:cutting_point]\n",
    "    \n",
    "    full_sentence = prefix+continuation\n",
    "\n",
    "    logits = get_logits(sess, run_name, checkpoint_dir, model_name, model_dir, full_sentence, all=True)\n",
    "\n",
    "    logits = logits[context_size-1:-1, :]  # only continuation logits\n",
    "    logitmeans = np.mean(logits, axis=1)\n",
    "    logits = logits - logitmeans[:, None]\n",
    "    explogits = np.exp(logits)\n",
    "    probabs = explogits / np.sum(explogits,axis=1)[:, None]\n",
    "    \n",
    "    probab_scores = np.nan_to_num([probabs[i, index] for i, index in enumerate(continuation_tokens)])\n",
    "    perplexity = 2 ** (-np.mean(np.log2(probab_scores)))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def generate_song_part(context, truncation, common_params_dict, savepath=None, min_length_stanza_chars=100,\n",
    "                       min_length_song_chars=500, chorus_needed=False):\n",
    "    \n",
    "    res = gpt2.generate(sess,\n",
    "              prefix=context,\n",
    "              truncate=truncation,\n",
    "              **common_params_dict\n",
    "              )\n",
    "    \n",
    "    good_results = []\n",
    "    for st in res:\n",
    "        print(f\"Length of generated chunk (chars) = {len(st) - len(context)}, (context len = {len(context)})\")\n",
    "        if len(st) - len(context) > min_length_stanza_chars:# and st.count(\"<RBEG>\") == 0:\n",
    "            st = st.replace(\"<RBEG>\", \"\").replace(\"<EOST>\", \"\")\n",
    "            good_results.append(st)\n",
    "            print(st)\n",
    "    print(f\"Out of {len(res)} initial stanzas, {len(good_results)} were left.\")\n",
    "          \n",
    "    save_count = 0\n",
    "    to_continue = []\n",
    "    if savepath:\n",
    "        for song in good_results:\n",
    "            if len(song) > min_length_song_chars:\n",
    "                if savepath == \"return\":\n",
    "                    return song\n",
    "                else:\n",
    "                    song_name = \"sample_\" + str(random.randrange(1_000_000_000))\n",
    "                    with open(savepath+song_name, \"w+\") as file:\n",
    "                        file.write(song)\n",
    "                        save_count += 1\n",
    "            else:\n",
    "                to_continue.append(song)\n",
    "        print(f\"{save_count} songs were saved.\")\n",
    "               \n",
    "            \n",
    "    for song in to_continue:\n",
    "        if chorus_needed:\n",
    "            context = song + \"<RBEG>\"\n",
    "            truncation = \"<EOST>\"\n",
    "            chorus_needed = False\n",
    "        else:\n",
    "            context = song\n",
    "            truncation = \"<EOST>\"\n",
    "        \n",
    "        generate_song_part(context=context, truncation=truncation, common_params_dict=common_params_dict,\n",
    "                           savepath=\"./samples/\", min_length_stanza_chars=min_length_stanza_chars,\n",
    "                           min_length_song_chars=min_length_song_chars, chorus_needed=chorus_needed)\n",
    "    \n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, name, mode, encoding=\"utf-8\"):\n",
    "        self.file = open(name, mode, encoding=encoding)\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "    def __del__(self):\n",
    "        sys.stdout = self.stdout\n",
    "        self.file.close()\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        try:\n",
    "            self.stdout.write(data)\n",
    "        except UnicodeEncodeError as err:\n",
    "            self.stdout.write(f\"Writing log didn't succeed due to {err}.\")\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "\n",
    "gpt2.get_perplexity = get_perplexity\n",
    "gpt2.get_logits = get_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8964183-5b4b-4da7-9fb5-b9435d55d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2.get_perplexity(sess,\n",
    "#                run_name=run_name,\n",
    "#                checkpoint_dir='checkpoint',\n",
    "#                model_name=\"124M\",\n",
    "#                model_dir=model_dir,\n",
    "#                prefix=prefix,\n",
    "#                continuation=\"generated\")\n",
    "\n",
    "\n",
    "\n",
    "common_params_dict = {\"run_name\":run_name, \"length\":100, \"temperature\":0.9,\n",
    "                      \"top_p\":0.95, \"return_as_list\":True, \"nsamples\":1, \"batch_size\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4d5b84-877a-42cd-aaa6-0f74320085bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': 'disco_polo_gen_gpt2_validation',\n",
       " 'length': 100,\n",
       " 'temperature': 0.9,\n",
       " 'top_p': 0.95,\n",
       " 'return_as_list': True,\n",
       " 'nsamples': 1,\n",
       " 'batch_size': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8865802d-e7a1-41b7-99df-d52fb7f3d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "Loading dataset...\n",
      "dataset has 3092002 tokens\n",
      "Training...\n",
      "Saving checkpoint/disco_polo_gen_gpt2_validation/model-1400\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.data-00000-of-00001\n",
      "INFO:tensorflow:497800\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.meta\n",
      "INFO:tensorflow:500900\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1400.index\n",
      "INFO:tensorflow:500900\n",
      "======== SAMPLE 1 ========\n",
      "askie głębi\n",
      "Miesię, kiedy do mnie, dzisiaj wielki zatańczą\n",
      "Tylko mi jest sama ją mnie,\n",
      "Jak wielki mam mieć\n",
      "<RBEG>Ref.\n",
      "<REND><REND><REND>\n",
      "<EOST>\n",
      "Hej hej, kiedy miała sama ją mnie,\n",
      "Miała sama ją mnie,\n",
      "Hej hej hej, kiedy świat mnie mą\n",
      "Łez tylko zatrzymała się skrycie\n",
      "Wielka mi mioła i ło żalny i łóży\n",
      "I choć nieracji mych złów, jak największe łży\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "<REND><REND><REND>.<EOST>.<|endoftext|>\n",
      "<|startoftext|>Tylko Ty uśmiechasz tylko jest\n",
      "Ty się wybrać jak mam zapomnieć\n",
      "Mówiłaś mi, że była zawożyła w duszy razem\n",
      "Ty się krótkę nawet szybciej bierz łatwo prawdę Cię\n",
      "Powiem jestem, że cię żal do przodu niej dzień\n",
      "Zapomnędą, że zacierać cichy zawędą\n",
      "<EOST>\n",
      "<RBEG>Ref: Tak nie dźwiętam, tam znów kród\n",
      "Nic nie nie wiem że tak ciało w sercu mam chcesz\n",
      "Tylko Ty i tylko jestem tak jak ja\n",
      "Wciąż mnie na mnie wspodają czy się ze mny jest<REND>\n",
      "<EOST>\n",
      "1.\n",
      "Hej hej hej, jak prawda mamy cudowni swe\n",
      "Że o tym chciałaś jak ogień i bardzo o tym biegło będzie ciebie\n",
      "Śmiet wybrać jak zawo mi brać\n",
      "<EOST>\n",
      "<RBEG>Ref: Tak nie dźwiętam, tam znów kród\n",
      "Nic nie nie wiem że tak ciało w sercu mam chcesz\n",
      "Tylko Ty i tylko jestem tak jak ja\n",
      "Wciąż mnie na mnie wspodają czy się ze mny jest   /z//<REND>\n",
      "<EOST>\n",
      "2.\n",
      "Hej hej jak kumpliwie się od nas być oczy\n",
      "Podolam sie na złość dal cógorski kum\n",
      "<EOST>\n",
      "<RBEG>Ref: Tak nie dźwiętam, tam znów kród\n",
      "Nic nie nie wiem że tak ciało w sercu mam chcesz\n",
      "Tylko Ty i tylko jestem tak jak ja\n",
      "Wciąż mnie na mnie wspodają czy się ze mny jest  <REND>a<EOST>.<|endoftext|>\n",
      "<|startoftext|>Gdy wysokoczmy wszystkie oczami\n",
      "Dni poczuć przy Tobie złoty mąc\n",
      "Lecz przyjacieciłaś większa chłopcy się\n",
      "Więcej wszystko tak włosach\n",
      "Wciąż śmieją Cię znów, co co, dziś nie moich dragnę Cię\n",
      "<EOST>\n",
      "Wysokóły chł\n",
      "\n",
      "[1410 | 195.74] loss=1.63 avg=1.63\n",
      "[1420 | 298.73] loss=1.29 avg=1.46\n",
      "[1430 | 400.39] loss=2.10 avg=1.68\n",
      "[1440 | 501.10] loss=1.59 avg=1.65\n",
      "[1450 | 601.55] loss=1.38 avg=1.60\n",
      "[1460 | 702.65] loss=1.84 avg=1.64\n",
      "[1470 | 803.95] loss=1.51 avg=1.62\n",
      "[1480 | 904.97] loss=1.37 avg=1.59\n",
      "[1490 | 1006.53] loss=1.78 avg=1.61\n",
      "[1500 | 1108.29] loss=1.73 avg=1.62\n",
      "Saving checkpoint/disco_polo_gen_gpt2_validation/model-1500\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1068: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1500.data-00000-of-00001\n",
      "INFO:tensorflow:497800\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1500.index\n",
      "INFO:tensorflow:497800\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-1500.meta\n",
      "INFO:tensorflow:500900\n",
      "[1510 | 1210.37] loss=1.20 avg=1.58\n",
      "[1520 | 1312.04] loss=1.62 avg=1.58\n",
      "[1530 | 1413.80] loss=1.34 avg=1.57\n",
      "[1540 | 1515.17] loss=1.54 avg=1.56\n",
      "[1550 | 1615.60] loss=1.82 avg=1.58\n",
      "[1560 | 1717.54] loss=1.27 avg=1.56\n",
      "[1570 | 1819.38] loss=2.53 avg=1.62\n",
      "[1580 | 1919.58] loss=1.94 avg=1.64\n",
      "[1590 | 2020.15] loss=1.59 avg=1.64\n",
      "[1600 | 2120.94] loss=2.44 avg=1.68\n",
      "======== SAMPLE 1 ========\n",
      "ie.\n",
      "<EOST>\n",
      "<RBEG>Ref. I pojawiam wtedy przodu dostępna\n",
      "Tęskoczy z rękę pokochałem\n",
      "Wieci zaś się będę sam\n",
      "Choć dziś dzień jest żart.\n",
      "\n",
      "Gdy wreszcie nie wiesz.\n",
      "Rozpierały pokochać sam ciał złe\n",
      "Słońce, rzeką za pokochać\n",
      "Jasną wciązać.\n",
      "\n",
      "A wolno znane, kiedy dzień.\n",
      "W sercu śnie miesiąc\n",
      "Uśmiech, w za na nic.\n",
      "\n",
      "I pojawiam wtedy przodu doste<REND>m<EOST>.<|endoftext|>\n",
      "<|startoftext|>1. Bierz wreszcie jest pamiędzy nie zapomnę, więc\n",
      "Żadne jest kładzę wciąż dzięknie wszystkie ty\n",
      "Bo wiesz co czas w szybko złądzeń nam swój czasem\n",
      "Nie powolóda mi, wyjół pozostań,\n",
      "Żadne pamiędzy jest pamiędzy nie opuszczę twój\n",
      "\n",
      "ref:\n",
      "Choć nie jest są jak głos,\n",
      "Jest najlepsza znów\n",
      "Choć nie jest jak wiem?\n",
      "Pewna Cię, słonko wielki skaczeć\n",
      "Choć nie jest najlepszego raj\n",
      "Choć wiesz już nie wiesz\n",
      "Chodź wszystkie co przysta\n",
      "Choć nie jest pamiędzy nie zapomnę\n",
      "Wszyscy jest tam kiedy mą jesteś Ty\n",
      "\n",
      "2. Powiedz poczu nie żało co tam jest\n",
      "Bo po zaczyła kiedy świat\n",
      "Zakazasz pójdą, gdy tego chcesz\n",
      "Odwracasz, gdy zobaczyła\n",
      "Zdradę, czas następna Cię\n",
      "Zanim kłudne do lubiącym życie które nas\n",
      "Udajesz mnie czujesz, szczytko ty\n",
      "\n",
      "ref:\n",
      "Choć nie jest są jak gło,\n",
      "Jest najlepsza znów\n",
      "Choć nie jest jak wiem?\n",
      "Pewna Cię, słonko wielki skaczeć\n",
      "Choć nie jest najlepszego raj\n",
      "Choć wiesz już nie wiesz\n",
      "Chodź wszystkie co przysta\n",
      "Choć nie jest pamiędzy nie zapomnę\n",
      "Wszyscy jest tam kiedy mą jesteś Ty\n",
      "\n",
      "3. Powiedz płoży do cień ich\n",
      "Szczęście co na kobiec pokochaż mnie\n",
      "Na ciepłych zaśnie bym szczęście\n",
      "Niespełniam świat, chociaż wcześnie mnie\n",
      "\n",
      "ref:\n",
      "Choć nie jest są jak gło,\n",
      "Jest najlepsza znów\n",
      "Choć nie jest przysta\n",
      "Rozpiersz wciąż, chociaż bierz co dajesz życie\n",
      "Ona do mnie ma tylko będę\n",
      "\n",
      "Ref:\n",
      "Hej dobrze włosy, to co chcesz\n",
      "Wszyscy wszystkie, wielk sk\n",
      "\n",
      "[1610 | 2306.79] loss=1.74 avg=1.69\n",
      "[1620 | 2407.05] loss=1.44 avg=1.67\n",
      "[1630 | 2509.02] loss=1.94 avg=1.69\n",
      "[1640 | 2610.06] loss=1.40 avg=1.67\n",
      "[1650 | 2710.71] loss=1.88 avg=1.68\n",
      "[1660 | 2812.04] loss=1.61 avg=1.68\n",
      "[1670 | 2913.68] loss=1.27 avg=1.66\n",
      "[1680 | 3015.61] loss=1.30 avg=1.65\n",
      "[1690 | 3116.11] loss=1.72 avg=1.65\n",
      "[1700 | 3217.59] loss=1.56 avg=1.65\n",
      "[1710 | 3318.67] loss=1.99 avg=1.66\n",
      "[1720 | 3419.19] loss=1.29 avg=1.65\n",
      "[1730 | 3520.50] loss=1.42 avg=1.64\n",
      "[1740 | 3621.93] loss=1.60 avg=1.64\n",
      "[1750 | 3721.51] loss=1.66 avg=1.64\n",
      "[1760 | 3821.98] loss=1.10 avg=1.62\n",
      "[1770 | 3924.00] loss=1.21 avg=1.61\n",
      "[1780 | 4026.05] loss=1.84 avg=1.61\n",
      "[1790 | 4128.67] loss=1.44 avg=1.61\n",
      "[1800 | 4230.79] loss=1.47 avg=1.60\n",
      "======== SAMPLE 1 ========\n",
      " w zawór znów mi daj.\n",
      "                          \n",
      "\n",
      "                        x2\n",
      "\n",
      "Tak się zawsze bywa w tego chcesz.\n",
      "                             x2\n",
      "             \n",
      "\n",
      "Tak się zawsze bywa w tego chcesz.\n",
      "                    x2\n",
      "                             x2\n",
      "                    x2\n",
      "                       x2\n",
      "       \n",
      "\n",
      "Chocia, chocia...\n",
      "          x2\n",
      "               x2\n",
      "         Lecz...\n",
      "                     x2\n",
      "          A... A...A... A... A... A...A...A...A...<EOST>x<|endoftext|>\n",
      "<|startoftext|>Dzisiaj pragniesz mieć  \n",
      "Zwariować się śmiech głos przed nami. \n",
      "Nagleść się jeszcze własne jest z tym, święc je. \n",
      "Dwa ci dał, lecz jak tym, z nich jóc \n",
      "Nie jesteś naktutniej dżeża się. \n",
      "Więc się w że co już całkiem już nie zmieni, Ty oczy wreszcie swoje sny. \n",
      "Chciałem żeby zbudzić chcę, gdy możesz takie źmienić, nad ranem sam. \n",
      "<EOST>\n",
      "<RBEG>Ref. Już do tego czas, noc się zmieni nas. Z krótka rytm ogarni, krótkowa chcę mieć, gdzie się uważasz mnie. \n",
      "Już do tego czas, noc się zmieni nas. Z krótka rytm ogarni, krótkowa chcę mieć, gdzie się uważasz mnie. \n",
      "Już do tego czas, noc się zmieni nas.<REND>\n",
      "<EOST>\n",
      "1 x 2\n",
      "Już do tego czas \n",
      "Tęsknię co jest piosenką jak się śmiało \n",
      "Żeby było mnie tą dziś już na świecie \n",
      "Jedno chce, co by zawsze będziesz mocno się \n",
      "Więc się w że co już całkiem już nie zmieni, Ty oczy wreszcie swoje sny. \n",
      "Chciałem żeby zbudzić chcę, gdy możesz takie źmienić, nad ranem sam. \n",
      "<EOST>\n",
      "<RBEG>Ref. Już do tego czas i nas zmieni tkwięci krok \n",
      "Taki jest raj, tylko jutro mocno jest. \n",
      "Już do tego czas i nas zmieni zapomniał że być nas zmien\n",
      "\n",
      "[1810 | 4418.31] loss=1.91 avg=1.61\n",
      "[1820 | 4521.91] loss=1.39 avg=1.61\n",
      "[1830 | 4624.97] loss=1.53 avg=1.60\n",
      "[1840 | 4730.11] loss=2.06 avg=1.62\n",
      "[1850 | 4834.11] loss=1.25 avg=1.61\n",
      "[1860 | 4940.00] loss=1.22 avg=1.60\n",
      "[1870 | 5044.11] loss=1.10 avg=1.58\n",
      "[1880 | 5148.69] loss=1.82 avg=1.59\n",
      "[1890 | 5252.00] loss=1.08 avg=1.58\n",
      "[1900 | 5354.79] loss=1.68 avg=1.58\n",
      "[1910 | 5456.79] loss=1.41 avg=1.57\n",
      "[1920 | 5560.04] loss=1.42 avg=1.57\n",
      "[1930 | 5663.16] loss=1.58 avg=1.57\n",
      "[1940 | 5765.00] loss=1.20 avg=1.56\n",
      "[1950 | 5870.01] loss=1.59 avg=1.56\n",
      "[1960 | 5975.83] loss=1.48 avg=1.56\n",
      "[1970 | 6080.33] loss=1.52 avg=1.56\n",
      "[1980 | 6182.86] loss=1.23 avg=1.55\n",
      "[1990 | 6285.63] loss=1.58 avg=1.55\n",
      "[2000 | 6388.46] loss=1.58 avg=1.55\n",
      "Saving checkpoint/disco_polo_gen_gpt2_validation/model-2000\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2000.data-00000-of-00001\n",
      "INFO:tensorflow:497800\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2000.meta\n",
      "INFO:tensorflow:500900\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2000.index\n",
      "INFO:tensorflow:500900\n",
      "======== SAMPLE 1 ========\n",
      "� szlakom kręci w dusie,\n",
      "To niezaskrzał następny, to za chwilę \n",
      "Zapada na takiej księci w ogóle,\n",
      "Dziś mnie jeszcze tak nam tego,\n",
      "Pomyślałem zapada na ta czuje,\n",
      "Razem znowu czem na szkodki i kręci na dziekami.\n",
      "\n",
      "Wieczory kacze sęs i nie zatrzyma dla siebie,\n",
      "Przysięka swe ciebie i bramy.\n",
      "Lecz tego wiosna i szczęście,\n",
      "To tylko prawdę, która o tym wierzyć,\n",
      "Wybierz wtedy to wzruszyć,\n",
      "I zostanie się, w mym takie zawsze być.\n",
      "\n",
      "Przez Ciebie kochasz, taki nami\n",
      "Czemu jeszcze cudnej piosenką świeci,\n",
      "Wszlą, za chwilę przez Ciebie do Ciebie.\n",
      "Razem tak nam tego, nie wytrzymam gwiazysz się,\n",
      "Kochać Cię z tego chcieć dać.\n",
      "\n",
      "Przez Ciebie kochasz! <EOST>\n",
      "<|startoftext|>Mogłeś dzisiaj moja muzykę,\n",
      "Gdyby od siebie jak razem,\n",
      "Wpadłem się z Tobą,\n",
      "To przecież o tym wiesz,\n",
      "Bo ty jesteś moja, moja, moja\n",
      "Bo ty jesteś moja, moja, moja, moja\n",
      "Czekam na mnie zaufasz \n",
      "Niezła to nie\n",
      "Moja moja muzykę / <EOST>6<|endoftext|>\n",
      "<|startoftext|><RBEG>Ref:\n",
      "Niepewności, szybko słowo\n",
      "Krążanych wódk\n",
      "Wszyscy to jest moja\n",
      "Tylko pragnieniem\n",
      "<REND>\n",
      "<EOST>\n",
      "Dziewczyno dzień, dziewczyno to\n",
      "W mych ramionach\n",
      "W mych świekę mam\n",
      "Nie ma pięknie tak\n",
      "Nie słyszysz i ść\n",
      "Nie zapomnę\n",
      "I wiem, że bardzo już\n",
      "I za sobą mieszka\n",
      "To wsadka\n",
      "To wsadka\n",
      "<EOST>\n",
      "<RBEG>Ref. x2\n",
      "Niepewności, szybko słowo\n",
      "Krążanych wódk\n",
      "Wszyscy to jest moja\n",
      "Tylko pragnieniem\n",
      "<REND>\n",
      "<EOST>\n",
      "Sądź właśnie do życia\n",
      "Nie potrzebny św\n",
      "Nie mogę jak i wciąż świejesz\n",
      "Widzę jeszcze mam\n",
      "Już zabrał w moich dni\n",
      "Kolorowe serca szczela\n",
      "Nie będziesz, nie będziesz\n",
      "<EOST>\n",
      "Nie pewności, szybko słowo\n",
      "Krążanych wódk\n",
      "Wszyscy to jest moja\n",
      "Tylko pragnieniem\n",
      "<REND>\n",
      "<EOST>\n",
      "Niepewności, szybko słowo\n",
      "Krążanych wódk\n",
      "Wszyscy to jest moja\n",
      "Tylko pragnieniem\n",
      "\n",
      "<RBEG>Ref.:\n",
      "Niepewności, szybko słowo\n",
      "Krą�\n",
      "\n",
      "[2010 | 6577.14] loss=1.52 avg=1.55\n",
      "[2020 | 6679.98] loss=1.38 avg=1.55\n",
      "[2030 | 6783.42] loss=1.44 avg=1.55\n",
      "[2040 | 6889.82] loss=1.71 avg=1.55\n",
      "[2050 | 6995.17] loss=1.11 avg=1.54\n",
      "[2060 | 7100.14] loss=1.75 avg=1.55\n",
      "[2070 | 7202.47] loss=1.48 avg=1.54\n",
      "[2080 | 7305.20] loss=2.31 avg=1.56\n",
      "[2090 | 7407.81] loss=1.61 avg=1.56\n",
      "[2100 | 7509.09] loss=1.92 avg=1.57\n",
      "[2110 | 7612.80] loss=1.37 avg=1.56\n",
      "[2120 | 7718.24] loss=1.14 avg=1.56\n",
      "[2130 | 7822.16] loss=1.65 avg=1.56\n",
      "[2140 | 7924.19] loss=1.69 avg=1.56\n",
      "[2150 | 8028.30] loss=1.70 avg=1.56\n",
      "[2160 | 8134.87] loss=1.54 avg=1.56\n",
      "[2170 | 8240.27] loss=1.77 avg=1.57\n",
      "[2180 | 8344.00] loss=1.09 avg=1.56\n",
      "[2190 | 8448.41] loss=1.68 avg=1.56\n",
      "[2200 | 8551.61] loss=1.42 avg=1.56\n",
      "======== SAMPLE 1 ========\n",
      "zyych rąk\n",
      "Nie moją zamie, lecz już na koniec nas\n",
      "Nie poddajcie, lecz jej słowa.\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem!\n",
      "\n",
      "Nie moją czeka, lecz na zasnęć swobodzi\n",
      "Nie moją zamie, lecz jej słowach dzieci\n",
      "Niedobusuje, niedobusuje teścią się\n",
      "Nie moją jestem, lecz ja to tylko z nami na rozgrzy\n",
      "Nie moją czeka chłódka, lecz komuś poczucie, udowi \n",
      "Nie moją szare chłódka\n",
      "I do niej i do zostało z nami\n",
      "Na niej, na zapatrzód\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem, lecz ja to tylko z nami na rozgrzy\n",
      "Nie moją czeka chłódka, lecz komuś poczucie, udowi \n",
      "Nie moją szare chłódka\n",
      "I do niej i do zostało z nami\n",
      "Na niej, na zapatrzód\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem, lecz ja to tylko z nami na rozgrzy\n",
      "Nie moją czeka chłódka, lecz komuś poczucie, udowi \n",
      "Nie moją szare chłódka\n",
      "I do niej i do zostało z nami\n",
      "Na niej, na zapatrzód (x2)\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem, lecz ja to tylko z nami na rozgrzy\n",
      "Nie moją czeka chłódka, lecz komuś poczucie, udowi \n",
      "Nie moją szare chłódka\n",
      "I do niej i do zostało z nami\n",
      "Na niej, na zapatrzód\n",
      "\n",
      "Nie moją jestem, lecz to już nie pomogła\n",
      "Nie mój, nie ma wznać, która jest!\n",
      "Nie moją jestem, lecz ja to tylko z nami na rozgrzy\n",
      "Nie moją czeka, lecz komuś poczucie, udowi \n",
      "Nie moją szare chłódka\n",
      "I do niej i do zostało z nami\n",
      "Na niej, na zapatrzód\n",
      "\n",
      "Nie moją jest\n",
      "\n",
      "[2210 | 8738.39] loss=1.26 avg=1.55\n",
      "[2220 | 8841.14] loss=1.84 avg=1.56\n",
      "[2230 | 8943.31] loss=1.18 avg=1.55\n",
      "[2240 | 9048.21] loss=1.53 avg=1.55\n",
      "[2250 | 9151.98] loss=1.77 avg=1.55\n",
      "[2260 | 9257.13] loss=2.04 avg=1.56\n",
      "[2270 | 9360.33] loss=1.29 avg=1.56\n",
      "[2280 | 9465.90] loss=1.39 avg=1.55\n",
      "[2290 | 9571.44] loss=1.66 avg=1.56\n",
      "[2300 | 9676.09] loss=1.53 avg=1.56\n",
      "[2310 | 9780.21] loss=1.41 avg=1.55\n",
      "[2320 | 9885.30] loss=1.67 avg=1.56\n",
      "[2330 | 9989.76] loss=1.58 avg=1.56\n",
      "[2340 | 10094.06] loss=1.02 avg=1.55\n",
      "[2350 | 10197.75] loss=1.42 avg=1.54\n",
      "[2360 | 10302.19] loss=1.29 avg=1.54\n",
      "[2370 | 10406.49] loss=1.62 avg=1.54\n",
      "[2380 | 10509.66] loss=1.02 avg=1.53\n",
      "[2390 | 10614.41] loss=1.00 avg=1.53\n",
      "[2400 | 10719.44] loss=1.07 avg=1.52\n",
      "======== SAMPLE 1 ========\n",
      "ie,\n",
      "I już nie mogę nigdy, już nie mogę żyć.\n",
      "Lato się nawet do ciebie,\n",
      "Gorą się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam.\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie.\n",
      "I dalej chcesz,\n",
      "Zabierz ze mną zachwistość.\n",
      "Wnet miłość nie mogę nigdy.\n",
      "Lato się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam<REND>\n",
      "<EOST>\n",
      "Gdy mieć nie mogę nigdy.\n",
      "Już nie mogę żyć.\n",
      "Wnet miłość nie mogę nigdy\n",
      "Lato się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam.\n",
      "<EOST>\n",
      "Jesteś pomarzyłem jak narkotyk znów.\n",
      "Tak zachwycałem mocno, że Ty uśmiechasz jej nocy.\n",
      "Nic nie płaczy,\n",
      "I gdyby źle zwariowałeś mi, nadzieje ma nóg.\n",
      "Gdyby noc jak ciebie,\n",
      "Gdyby krok zaśpiewają się mam.\n",
      "Lato się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam.\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie.\n",
      "I dalej chcesz,\n",
      "Zabierz ze mną zachwistość.\n",
      "Wnet miłość nie mogę nigdy.\n",
      "Lato się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam.\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie.\n",
      "I dalej chcesz,\n",
      "Zabierz ze mną zachwistość.\n",
      "Wnet miłość nie mogę nigdy.\n",
      "Lato się zaczął za to i życie<REND>\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie<REND>\n",
      "<EOST>\n",
      "<RBEG>Ref.\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie.\n",
      "I dalej chcesz,\n",
      "Zabierz ze mną zachwistość.\n",
      "Wnet miłość nie mogę nigdy.\n",
      "Lato się zaczął za to i życie.\n",
      "Lato się, do tego wszystkim sam.\n",
      "<EOST>\n",
      "Tylko chcesz,\n",
      "Gdy miejdż po rzyku mam \n",
      "Tęskniesz mi życie<EOST>.\n",
      "I dalej chcesz,\n",
      "Zabierz ze mną zachwistość.\n",
      "Wnet miłość nie mogę nigdy.\n",
      "Lato się\n",
      "\n",
      "[2410 | 10908.96] loss=1.78 avg=1.52\n",
      "[2420 | 11012.89] loss=1.52 avg=1.52\n",
      "[2430 | 11116.84] loss=1.50 avg=1.52\n",
      "[2440 | 11218.54] loss=1.72 avg=1.52\n",
      "[2450 | 11321.69] loss=1.94 avg=1.53\n",
      "[2460 | 11424.28] loss=0.95 avg=1.52\n",
      "[2470 | 11525.16] loss=2.05 avg=1.53\n",
      "[2480 | 11627.49] loss=1.12 avg=1.52\n",
      "[2490 | 11730.06] loss=1.58 avg=1.53\n",
      "[2500 | 11832.19] loss=1.55 avg=1.53\n",
      "Saving checkpoint/disco_polo_gen_gpt2_validation/model-2500\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2500.meta\n",
      "INFO:tensorflow:3100\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2500.data-00000-of-00001\n",
      "INFO:tensorflow:500900\n",
      "INFO:tensorflow:checkpoint/disco_polo_gen_gpt2_validation/model-2500.index\n",
      "INFO:tensorflow:500900\n",
      "[2510 | 11934.60] loss=1.93 avg=1.53\n",
      "[2520 | 12037.23] loss=1.23 avg=1.53\n",
      "[2530 | 12139.07] loss=1.20 avg=1.52\n",
      "[2540 | 12239.90] loss=1.20 avg=1.52\n",
      "[2550 | 12341.13] loss=1.80 avg=1.52\n",
      "[2560 | 12442.64] loss=1.35 avg=1.52\n",
      "[2570 | 12543.41] loss=1.29 avg=1.52\n",
      "[2580 | 12644.29] loss=1.97 avg=1.52\n",
      "[2590 | 12745.14] loss=1.29 avg=1.52\n",
      "[2600 | 12845.18] loss=1.50 avg=1.52\n",
      "======== SAMPLE 1 ========\n",
      "aniach święcam, przyznam się.\n",
      "Zapachem pomyślę, ktoś mnie czas znika i będzie na mnie.\n",
      "\n",
      "<RBEG>Ref: Ja wiem, dobrze dni nie wiem, że tak są dłoń się nie chcę.\n",
      "Ale kocham Cię, ciągle jestem w nas, że ja cię chcę.\n",
      "\n",
      "2.\tOna piękna mi dziewczyna przy mnie mocno tak\n",
      "Księżyc wolność i łzy, była to wszystko dzisiaj zna.\n",
      "Więc dobrze żeby znać nie może ta zmieni się.\n",
      "O, o, odchodzisz mi mamy się, ja kocham się i ty, nie wiem nie jest mi dajesz życia\n",
      "\n",
      "<RBEG>Ref: Ja wiem, dobrze dni nie wiem, że tak są dłoń się nie chcę.\n",
      "Ale kocham Cię, ciągle jestem w nas, że ja cię chcę. <REND><REND>x<EOST>2<|endoftext|>\n",
      "<|startoftext|>1. Dzieweczysz już ostatni mam,\n",
      "W sercu cudowne dni,\n",
      "Mogłaby się, zaczął się.\n",
      "Wyciągnij więc nie odchodzisz mam.\n",
      "I nie ma nas, jakby mam.\n",
      "<EOST>\n",
      "Ref x 2\n",
      "Cóż zawsze chcę,\n",
      "Niech odwrócisz się,\n",
      "Miłość już nie jest mój,\n",
      "Dotyk Twa słowa tej.\n",
      "<EOST>\n",
      "Ref x4\n",
      "Cóż zawse chcę,\n",
      "Niech odwrócisz się,\n",
      "Miłość już nie jest mój,\n",
      "Dotyk Twa słowa tej.\n",
      "<EOST>\n",
      "2. Już kiedy pamiętasz w niej przy mnie\n",
      "Co się stało do Ciebie,\n",
      "Nie dajesz pamiętasz nam.\n",
      "Właśnie dolcisz chcesz mam.\n",
      "<EOST>\n",
      "Ref x 4\n",
      "Cóż zawse chcę...\n",
      "<EOST>\n",
      "Ref x 4\n",
      "Cóż zaws<EOST>z<|endoftext|>\n",
      "<|startoftext|>1.\tChcę Cię jak prawo\n",
      "Żegnaj przez gwiazd, nie wiem sam\n",
      "Nie licz się w żyłach słońca\n",
      "To Ty mam, ja zaprawię Ci\n",
      "Co mam za to, że Ty\n",
      "<EOST>\n",
      "<RBEG>Ref: Jedziemy jesteśmy, gdzieś w błąk\n",
      "A Ty miła w nim tych wól.\n",
      "To taka wie, nie wiadomo dnia\n",
      "Gdyby namiętna by tak źli\n",
      "Jedziemy jesteśmy, rzucić się żonnie łez\n",
      "Jedziesz, jedziesz miłość, jeden Twoje płacze / x2<REND>\n",
      "<EOST>\n",
      "2.\tA w oczach jest dzień\n",
      "I mamy znaleźć się, czy możesz śmieszne jest\n",
      "Były słowo dziś się tylko z nim?\n",
      "<EOST>\n",
      "<RBEG\n",
      "\n",
      "[2610 | 13030.78] loss=1.43 avg=1.52\n",
      "[2620 | 13131.94] loss=1.47 avg=1.52\n",
      "[2630 | 13233.96] loss=1.30 avg=1.51\n",
      "[2640 | 13335.63] loss=1.08 avg=1.51\n",
      "[2650 | 13436.06] loss=1.65 avg=1.51\n",
      "[2660 | 13537.62] loss=1.02 avg=1.50\n",
      "[2670 | 13640.35] loss=1.62 avg=1.50\n",
      "[2680 | 13740.87] loss=1.59 avg=1.51\n",
      "[2690 | 13842.10] loss=1.59 avg=1.51\n",
      "[2700 | 13945.18] loss=2.21 avg=1.52\n",
      "[2710 | 14045.61] loss=2.50 avg=1.53\n",
      "[2720 | 14146.86] loss=1.55 avg=1.53\n",
      "[2730 | 14249.13] loss=1.01 avg=1.52\n",
      "[2750 | 14451.87] loss=1.13 avg=1.53\n",
      "[2760 | 14553.14] loss=1.52 avg=1.53\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.stdout = Tee(\"log_data2.dat\", mode=\"a\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b8f098-1193-4aa7-bc4d-c175136ba5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Testing saving output...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e50eaa-4092-42b6-87ee-844b1029f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 07:19:22.092399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-26 07:19:22.168660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-26 07:19:22.168716: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-26 07:19:22.168751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pytorch-1-10-20220326-144724): /proc/driver/nvidia/version does not exist\n",
      "2022-05-26 07:19:32.335589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for song in validation.song:\n",
    "#        splitted = song.split(\"\\n\")\n",
    "        #print(splitted)\n",
    "#        context = splitted[0]\n",
    "#        del splitted[0]\n",
    "#        continuation = \"\".join(splitted)\n",
    "        #print(song)\n",
    "        #print(continuation)\n",
    "        #print(context)\n",
    "sess = gpt2.start_tf_sess() \n",
    "\n",
    "gpt2.finetune(sess,\n",
    "                  model_dir=model_dir,\n",
    "                  dataset=file_name,\n",
    "                  model_name=model_name,\n",
    "                  reuse=False,\n",
    "                  steps=8000,\n",
    "                  run_name=run_name,\n",
    "                  print_every=10,\n",
    "                  sample_every=200,\n",
    "                  save_every=500, \n",
    "                  only_train_transformer_layers=False,\n",
    "                  accumulate_gradients = 1,\n",
    "                  overwrite=True,\n",
    "                  restore_from=\"latest\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c5d72a-0e10-45f4-9fab-434a0c319ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 18:51:51.040688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-25 18:51:51.114893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-25 18:51:51.114971: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-25 18:51:51.115003: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pytorch-1-10-20220326-144724): /proc/driver/nvidia/version does not exist\n",
      "2022-05-25 18:52:02.780146: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-05-25 18:52:04.525451: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.158562: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.336886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.652409: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2022-05-25 18:52:05.874948: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "2022-05-25 18:54:32.017068: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cast_op.cc:109 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n",
      "2022-05-25 18:54:32.123658: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cwise_ops_common.cc:81 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradients/model/h10/attn/Max_grad/Cast' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n      restore_from=\"latest\"\n    File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n      opt_grads = tf.gradients(ys=loss, xs=train_vars)\nNode: 'gradients/model/h10/attn/Max_grad/Cast'\nOOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\nOriginal stack trace for 'gradients/model/h10/attn/Max_grad/Cast':\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n    opt_grads = tf.gradients(ys=loss, xs=train_vars)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 314, in gradients_v2\n    unconnected_gradients)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 328, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 241, in _MaxGrad\n    return _MinOrMaxGrad(op, grad)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 231, in _MinOrMaxGrad\n    indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1002, in cast\n    x = gen_math_ops.cast(x, base_type, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2015, in cast\n    \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3762, in _create_op_internal\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\n...which was originally created as op 'model/h10/attn/Max', defined at:\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus, reuse=reuse)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 203, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 139, in attn\n    a = multihead_attn(q, k, v)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 127, in multihead_attn\n    w = softmax(w)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 51, in softmax\n    x = x - tf.reduce_max(input_tensor=x, axis=axis, keepdims=True)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3105, in reduce_max\n    _ReductionDims(input_tensor, axis))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3116, in reduce_max_with_dims\n    gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 6114, in _max\n    name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2597/462499367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0maccumulate_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                   \u001b[0mrestore_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                   )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 (_, v_loss, v_summary) = sess.run(\n\u001b[1;32m    344\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mopt_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     feed_dict={context: sample_batch()})\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0msummary_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1396\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradients/model/h10/attn/Max_grad/Cast' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n      restore_from=\"latest\"\n    File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n      opt_grads = tf.gradients(ys=loss, xs=train_vars)\nNode: 'gradients/model/h10/attn/Max_grad/Cast'\nOOM when allocating tensor with shape[1,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/model/h10/attn/Max_grad/Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\nOriginal stack trace for 'gradients/model/h10/attn/Max_grad/Cast':\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 236, in finetune\n    opt_grads = tf.gradients(ys=loss, xs=train_vars)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 314, in gradients_v2\n    unconnected_gradients)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 328, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\", line 696, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 241, in _MaxGrad\n    return _MinOrMaxGrad(op, grad)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\", line 231, in _MinOrMaxGrad\n    indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1002, in cast\n    x = gen_math_ops.cast(x, base_type, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2015, in cast\n    \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3762, in _create_op_internal\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\n...which was originally created as op 'model/h10/attn/Max', defined at:\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"/tmp/ipykernel_2597/462499367.py\", line 24, in <module>\n    restore_from=\"latest\"\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus, reuse=reuse)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 203, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 139, in attn\n    a = multihead_attn(q, k, v)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 127, in multihead_attn\n    w = softmax(w)\n  File \"/opt/conda/lib/python3.7/site-packages/gpt_2_simple/src/model.py\", line 51, in softmax\n    x = x - tf.reduce_max(input_tensor=x, axis=axis, keepdims=True)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3105, in reduce_max\n    _ReductionDims(input_tensor, axis))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3116, in reduce_max_with_dims\n    gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 6114, in _max\n    name=name)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 799, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess() \n",
    "steps = 200\n",
    "scores = []\n",
    "n_steps = []\n",
    "i = 0\n",
    "\n",
    "while i < 3 or (scores[-1] + scores[-2])/2 < scores[-3]:\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    # run model for 200 steps\n",
    "    gpt2.finetune(sess,\n",
    "                  model_dir=model_dir,\n",
    "                  dataset=file_name,\n",
    "                  model_name=model_name,\n",
    "                  reuse=False,\n",
    "                  steps=steps,\n",
    "                  run_name=run_name,\n",
    "                  print_every=10,\n",
    "                  sample_every=200,\n",
    "                  save_every=500, \n",
    "                  only_train_transformer_layers=False,\n",
    "                  accumulate_gradients = 1,\n",
    "                  overwrite=True,\n",
    "                  restore_from=\"latest\"\n",
    "                  )\n",
    "    \n",
    "    # calculate perplexity on test set\n",
    "    print(\"Calculating perplexity...\")\n",
    "    tmp_perplexity = []\n",
    "    ids = np.random.choice(range(len(validation.song)), size=100, replace=False)\n",
    "    sampled_100_songs = validation.song.loc[ids]\n",
    "    for song in tqdm(sampled_100_songs):\n",
    "        splitted = song.split(\"\\n\")\n",
    "        context = splitted[0]\n",
    "        first_eost = song.find(\"<EOST>\")\n",
    "        if first_eost < len(context):\n",
    "            first_eost = min(2 * len(context), len(song))\n",
    "            \n",
    "        continuation = song[len(context):first_eost]\n",
    "        \n",
    "        #sample = generate_song_part(context=context, truncation=\"<RBEG>\", common_params_dict=common_params_dict,\n",
    "        #           savepath=\"return\", min_length_stanza_chars=40, min_length_song_chars=700,\n",
    "        #           chorus_needed=True)\n",
    "        #print(\"Calculating perplexity...\")\n",
    "        perp_i = 0\n",
    "        try:\n",
    "            perp_i = gpt2.get_perplexity(sess,\n",
    "                       run_name=run_name,\n",
    "                       checkpoint_dir='checkpoint',\n",
    "                       model_name=\"124M\",\n",
    "                       model_dir=model_dir,\n",
    "                       prefix=context,\n",
    "                       continuation=continuation)\n",
    "        except  Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        tmp_perplexity.append(perp_i)\n",
    "        \n",
    "    perp = sum(tmp_perplexity) / len(validation)\n",
    "    scores.append(perp)\n",
    "    print(f\"Current perplexity = {perp}\")\n",
    "    # update iterator for stop condition\n",
    "    i += 1\n",
    "    \n",
    "print(\"Training done.\")\n",
    "print(f\"We did {(i+1) * 200} steps in total.\")\n",
    "print(\"Perplexity scores:\")\n",
    "print(scores)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./validation_info.txt\", \"w+\") as f:\n",
    "    f.write(f\"We did {(i+1) * 200} steps in total.\")\n",
    "    f.write(\"Perplexity scores:\")\n",
    "    f.write(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9cc4f-634f-42a3-9dc0-a077d1c628fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e46fec-ea15-4d66-a7ea-37ef01ea7b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3760be-c41b-4ab8-9986-a74efec9731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4327aa-ad91-456f-9e5a-a7f4e15702f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
