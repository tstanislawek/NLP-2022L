{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366c4184-0756-4417-90a1-c3b968ceca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361cb86b-7ae8-4f8e-9cd7-d5bf5bc0bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir('/home/jupyter/NLP_disco_polo_generator/samples') if 'sample' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437d3714-2ccc-426d-8a47-cf5193db547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    with open('/home/jupyter/NLP_disco_polo_generator/samples/' + file) as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a79854-05e9-48cd-80bd-28467353c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = int(len(texts)*0.8)\n",
    "texts_train = texts[:b]\n",
    "texts_test = texts[b::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0e6689-e8e9-4049-b972-6611ff9812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(texts_train).to_csv('generated_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae93def-4053-4d4c-91ba-31f3e6eaed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(texts_test).to_csv('generated_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfbfe4-3e52-448c-808b-6742ff1e9351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef43672-2a86-42ec-9a1c-1a10a9bac97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac5a750-bd8c-43ed-9bf4-1a5294bcb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_not_generated = pd.read_csv('train.csv')\n",
    "train_not_generated.columns = ['text']\n",
    "train_not_generated['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35be0184-0924-4133-b13c-c0c379b01a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm = pd.concat([pd.DataFrame({'text':texts_train, 'label':[1]*len(texts_train)}), train_not_generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312294b1-d3d5-494c-a442-6f6f89633ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_generated = pd.read_csv('test.csv')\n",
    "test_not_generated.columns = ['text']\n",
    "test_not_generated['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d02571-d144-4a10-b614-91b7ed558f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm = pd.concat([pd.DataFrame({'text':texts_test, 'label':[1]*len(texts_test)}), test_not_generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42003661-46cb-457c-a10f-0fb8df2360b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm.to_csv('/home/jupyter/NLP_disco_polo_generator/MS/lstm_train.csv', index = False)\n",
    "test_lstm.to_csv('/home/jupyter/NLP_disco_polo_generator/MS/lstm_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e0cd678-77d1-4d89-96ac-87fe2d6ac96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85bf68c3-d2a9-4b98-bce5-bd613d21c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-051d85e44f2ae552\n",
      "Reusing dataset csv (/home/jupyter/.cache/huggingface/datasets/csv/default-051d85e44f2ae552/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c04c2fe355245e3a920005312aa3bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": '/home/jupyter/NLP_disco_polo_generator/MS/lstm_train.csv',\n",
    "                                     \"test\": '/home/jupyter/NLP_disco_polo_generator/MS/lstm_test.csv'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2628ad2-b46f-49b8-84ea-3c8d1d639ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018d7ba2-0ad6-4b83-a73d-2af820f348de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(train_lstm['text'].values, tf.float32),\n",
    "            tf.cast(train_lstm['label'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d063376c-f583-468c-9314-6a0282663440",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(test_lstm['text'].values, tf.float32),\n",
    "            tf.cast(test_lstm['label'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0efc9332-7df2-43a8-90da-b3fd3e23e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d7faae0-9e03-400c-b427-a6557c0c4181",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`adapt` is only supported in tensorflow v2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15508/1963525688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m encoder = tf.keras.layers.TextVectorization(\n\u001b[1;32m      3\u001b[0m     max_tokens=VOCAB_SIZE)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    426\u001b[0m           \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0marray\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adapt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_use_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`adapt` is only supported in tensorflow v2.'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=g-doc-exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compile with defaults.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `adapt` is only supported in tensorflow v2."
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
