{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ur775rswQFm"
   },
   "source": [
    "# **Goal**: Reproducing some results from the paper [Confident Learning: Estimating Uncertainty in Dataset Labels](https://arxiv.org/abs/1911.00068) (JAIR, 2021)\n",
    "\n",
    "**Method**: https://github.com/cleanlab/cleanlab\n",
    "\n",
    "**Data**: Amazon Reviews 5-core (9.9gb, 2014) http://jmcauley.ucsd.edu/data/amazon/links.html *// comment: there is a newer (14.3gb, 2018)*\n",
    "\n",
    "**Example**: https://github.com/cleanlab/examples/blob/master/amazon-reviews-fasttext/amazon_pyx.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai-cUI2ca2pP"
   },
   "source": [
    "**Notes**:\n",
    "- JAIR 2019 paper only mentions (without code):\n",
    "> To demonstrate that non-deep-learning methods can be effective in\n",
    "finding label issues under the CL framework, we use a multinomial logistic regression classifier for both finding label errors and learning with noisy labels. The built-in SGD optimizer in the open-sourced fastText library (Joulin et al., 2017) is used with settings: initial learning rate = 0.1, embedding dimension = 100, and n-gram = 3). Out-of-sample predicted probabilities Confident Learning: Estimating Uncertainty in Dataset Labels are obtained via 5-fold cross-validation. For input during training, a review is represented as the mean of pre-trained, tri-gram, word-level fastText embeddings (Bojanowski et al., 2017).\n",
    "- [NeurIPS 2021 paper](https://openreview.net/forum?id=XccDXrDNLek) only mentions using fastText. To reproduce the results, the paper shares the predicted probabilities, but does not share the code used to obtain these predictions (https://github.com/cleanlab/label-errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be download from http://jmcauley.ucsd.edu/data/amazon/links.html or accessed from https://drive.google.com/uc?id=1W0B5KjjLBRRPBk0M4Pzoh_zFZ72oGtCr using https://stackoverflow.com/a/50670037."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## amazon5core.txt\n",
    "# url = \"https://drive.google.com/uc?id=1W0B5KjjLBRRPBk0M4Pzoh_zFZ72oGtCr\"\n",
    "# gdown.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kcore_5_helpful.csv\n",
    "# url = \"https://drive.google.com/uc?id=1-4t7iJXOh-PJnzIxaVWv5Mo_oRoQ2bln\"\n",
    "# gdown.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "    \n",
    "    \n",
    "def get_dataframe(path):\n",
    "    # obtain a dataset as pandas.DataFrame for exploration\n",
    "    dict_for_df = {}\n",
    "    for i, d in enumerate(tqdm(get_json(path))):\n",
    "         dict_for_df[i] = d\n",
    "    return pd.DataFrame.from_dict(dict_for_df, orient='index')\n",
    "\n",
    "\n",
    "def create_dataframe_helpful(path):\n",
    "    # obtain a dataset as pandas.DataFrame for exploration\n",
    "    # as defined in the paper, we take only reviews with helpfullness ratio > 0.5\n",
    "    \n",
    "    df = {}\n",
    "    for i, d in enumerate(get_json(path)):\n",
    "        h = d['helpful']\n",
    "        if h[0] > h[1] // 2: \n",
    "            df[i] = {'rating': d['overall'], 'text': d['reviewText']}\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "def create_dataset(path_input, path_output, n_rows=None, verbose=True):\n",
    "    # create a dataset used in the paper\n",
    "    # - take only reviews with helpfullness ratio > 0.5\n",
    "    # - and for classes: 1, 3, 5\n",
    "    # `n_rows` allows to reduce the number of reviews for prototyping\n",
    "    \n",
    "    labels = []\n",
    "    iterator = tqdm(get_json(path_input)) if verbose else get_json(path_input)\n",
    "    j = 0\n",
    "    with open(path_output+\".txt\", \"w\") as f:\n",
    "        for i, d in enumerate(iterator):\n",
    "            h = d[\"helpful\"]\n",
    "            if h[0] > h[1] // 2:\n",
    "                label = int(d[\"overall\"])\n",
    "                if label in [1, 3, 5]:\n",
    "                    text = d[\"reviewText\"]\n",
    "                    if len(text) > 0:\n",
    "                        f.write(\n",
    "                            \"__label__{} {}\\n\".format(\n",
    "                                label,\n",
    "                                text.strip().replace(\"\\n\", \" __newline__ \"),\n",
    "                            )\n",
    "                        )\n",
    "                        labels.append(label)\n",
    "                    j += 1\n",
    "                    if n_rows:\n",
    "                        if j == n_rows:\n",
    "                            break\n",
    "\n",
    "    label_map = {1: 0, 3: 1, 5: 2}\n",
    "    labels = [label_map[l] for l in labels]\n",
    "    with open(path_output+\".npy\", \"wb\") as g:\n",
    "        np.save(g, np.array(labels))\n",
    "    \n",
    "    if verbose:\n",
    "        print(pd.Series(labels).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe of all the reviews for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_json at 0x7f9bcf6e9230>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_json('data/kcore_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = create_dataframe_helpful('data/kcore_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/kcore_5_helpful.csv\")\n",
    "df = pd.read_csv(\"data/kcore_5_helpful.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13406523 entries, 0 to 13406522\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   Unnamed: 0  int64  \n",
      " 1   rating      float64\n",
      " 2   text        object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 306.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this for my husband who plays the pia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This work bears deep connections to themes fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You may laugh, but I have found that Otherland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Do not try and vacuum the dust. That's impossi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What if Dread had come out victorious and left...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rating                                               text\n",
       "0           1     5.0  I bought this for my husband who plays the pia...\n",
       "1           5     4.0  This work bears deep connections to themes fir...\n",
       "2           6     5.0  You may laugh, but I have found that Otherland...\n",
       "3           7     5.0  Do not try and vacuum the dust. That's impossi...\n",
       "4           8     5.0  What if Dread had come out victorious and left..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the final dataset used in the fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    \"amazon5core_mini\": 100_000,\n",
    "    \"amazon5core_medium\": 1_000_000,\n",
    "    \"amazon5core\": None # 10mln\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345162it [00:13, 25100.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    77416\n",
      "1    15120\n",
      "0     7442\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3834235it [02:23, 26777.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    756758\n",
      "1    163119\n",
      "0     79963\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41135700it [21:57, 31229.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    7890518\n",
      "1    1188544\n",
      "0     917375\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for file_name, n_rows in dataset_metadata.items():\n",
    "    create_dataset(path_input='data/kcore_5.json.gz', path_output=\"data/\"+file_name, n_rows=n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create models with fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mini (100k samples): `gdown.download(https://drive.google.com/uc?id=1Z-caK6XskdisHH_lWakxxPJBMoSHmUOH)`\n",
    "- medium (1mln samples): `gdown.download(https://drive.google.com/uc?id=1-7HCceJ5cD2AdfrMR1sOGf3zYo_vIA23)`\n",
    "- full dataset (10mln samples): `gdown.download(https://drive.google.com/uc?id=1UkjeSBXkovxlD1zFbRF_idZdpMeOVdpt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZDJnrvlDYHgP"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    for i, (N, p, r) in enumerate(results):\n",
    "        print(\"Precision@{}\\t{:.3f}\".format(i, p))\n",
    "        print(\"Recall@{}\\t{:.3f}\".format(i, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate (on train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9xNGxbUXKubZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 20M words\n",
      "Number of words:  690059\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  662558 lr:  0.000000 avg.loss:  0.365555 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplary words used in the model: ['the', 'and', 'of', 'to', 'a', 'is', 'in', 'I', 'that', 'this']\n",
      "Labels, targets used in the model: ['__label__5', '__label__3', '__label__1']\n",
      "Precision@0\t0.906\n",
      "Recall@0\t0.906\n",
      "Precision@1\t0.492\n",
      "Recall@1\t0.983\n",
      "Precision@2\t0.492\n",
      "Recall@2\t0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 204M words113M words\n",
      "Number of words:  3759048\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  677459 lr:  0.000000 avg.loss:  0.181880 ETA:   0h 0m 0s 21.4% words/sec/thread:  679532 lr:  0.078646 avg.loss:  0.348825 ETA:   0h11m51s 34.8% words/sec/thread:  682991 lr:  0.065186 avg.loss:  0.302580 ETA:   0h 9m46s 82.2% words/sec/thread:  675618 lr:  0.017752 avg.loss:  0.204062 ETA:   0h 2m41s 95.6% words/sec/thread:  679243 lr:  0.004391 avg.loss:  0.187117 ETA:   0h 0m39s 96.8% words/sec/thread:  678291 lr:  0.003155 avg.loss:  0.185442 ETA:   0h 0m28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplary words used in the model: ['the', 'and', 'of', 'to', 'a', 'is', 'in', 'I', 'that', 'this']\n",
      "Labels, targets used in the model: ['__label__5', '__label__3', '__label__1']\n",
      "Precision@0\t0.980\n",
      "Recall@0\t0.980\n",
      "Precision@1\t0.499\n",
      "Recall@1\t0.999\n",
      "Precision@2\t0.499\n",
      "Recall@2\t0.999\n"
     ]
    }
   ],
   "source": [
    "times = {'index': [], 'train': [], 'inference': []}\n",
    "\n",
    "for path_train in list(dataset_metadata):\n",
    "    \n",
    "    times['index'].append(path_train)\n",
    "    \n",
    "    st = time.time()\n",
    "    model = fasttext.train_supervised(\n",
    "        input=f'data/{path_train}.txt', \n",
    "        lr=0.1,\n",
    "        dim=100,\n",
    "        wordNgrams=3,\n",
    "        epoch=6,\n",
    "        thread=2,\n",
    "        verbose=2\n",
    "    )\n",
    "    et = time.time()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    times['train'].append(et - st)\n",
    "        \n",
    "    print(f'Exemplary words used in the model: {model.words[0:10]}')\n",
    "    print(f'Labels, targets used in the model: {model.labels}')\n",
    "    \n",
    "    st = time.time()\n",
    "    result_1 = model.test(f'data/{path_train}.txt', 1)\n",
    "    et = time.time()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    result_3 = model.test(f'data/{path_train}.txt', 2)\n",
    "    result_5 = model.test(f'data/{path_train}.txt', 3)\n",
    "    \n",
    "    times['inference'].append(et - st)\n",
    "    \n",
    "    print_results((result_1, result_3, result_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SIl0k3lpqADH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>train</th>\n",
       "      <th>inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon5core_mini</td>\n",
       "      <td>98.159207</td>\n",
       "      <td>21.145966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon5core_medium</td>\n",
       "      <td>950.633152</td>\n",
       "      <td>204.628172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index       train   inference\n",
       "0    amazon5core_mini   98.159207   21.145966\n",
       "1  amazon5core_medium  950.633152  204.628172"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_times = pd.DataFrame(times)\n",
    "df_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They trained using only 1mln samples, 10mln samples takes ~2h to train, which we did before but omit here (takes too long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOUb9p7LmSpG",
    "tags": []
   },
   "source": [
    "## Clean data with cleanlab\n",
    "\n",
    "We use the implementation of `fasttext` available in the `cleanlab` package, which automizes crossvalidation, as we need unbiased probabilties for Confident Learning.\n",
    "\n",
    "We use parameters mentioned in the JAIR paper (slightly different from the `cleanlab` example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from cleanlab.experimental.fasttext import FastTextClassifier, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_n_folds = 5  # Increasing more improves pyx, at great cost.\n",
    "seed = 0\n",
    "lr = 0.1\n",
    "ngram = 3\n",
    "epochs = 5  # Increasing more doesn't do much.\n",
    "dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"data/amazon5core_medium.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XuIIAFBiikI_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 163M words\n",
      "Number of words:  3197476\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  756192 lr:  0.000000 avg.loss:  0.242466 ETA:   0h 0m 0s  4.3% words/sec/thread:  711662 lr:  0.095739 avg.loss:  0.509927 ETA:   0h 9m11s 13.1% words/sec/thread:  746226 lr:  0.086938 avg.loss:  0.411464 ETA:   0h 7m57s 14.2% words/sec/thread:  744752 lr:  0.085850 avg.loss:  0.405121 ETA:   0h 7m52s 0.381876 ETA:   0h 6m57s 24.1% words/sec/thread:  773555 lr:  0.075937 avg.loss:  0.375495 ETA:   0h 6m42s 24.6% words/sec/thread:  777169 lr:  0.075435 avg.loss:  0.374526 ETA:   0h 6m37s 58.2% words/sec/thread:  770592 lr:  0.041759 avg.loss:  0.293800 ETA:   0h 3m41s 61.2% words/sec/thread:  772192 lr:  0.038752 avg.loss:  0.289431 ETA:   0h 3m25s\n",
      "Read 164M words\n",
      "Number of words:  3201514\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  741324 lr:  0.000000 avg.loss:  0.241752 ETA:   0h 0m 0s 0.069477 avg.loss:  0.352077 ETA:   0h 6m36s 51.4% words/sec/thread:  742801 lr:  0.048567 avg.loss:  0.305337 ETA:   0h 4m28s\n",
      "Read 163M words\n",
      "Number of words:  3198292\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  747016 lr:  0.000000 avg.loss:  0.243433 ETA:   0h 0m 0s 98.3% words/sec/thread:  748534 lr:  0.001710 avg.loss:  0.245128 ETA:   0h 0m 9s\n",
      "Read 164M words159M words\n",
      "Number of words:  3202789\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  752405 lr:  0.000000 avg.loss:  0.241255 ETA:   0h 0m 0s 670304 lr:  0.097110 avg.loss:  0.540781 ETA:   0h 9m54s% words/sec/thread:  748524 lr:  0.076534 avg.loss:  0.376053 ETA:   0h 6m59s 31.2% words/sec/thread:  751641 lr:  0.068770 avg.loss:  0.350165 ETA:   0h 6m15s 0.063516 avg.loss:  0.335123 ETA:   0h 5m44s  0h 0m52s\n",
      "Read 164M words\n",
      "Number of words:  3202903\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  737276 lr:  0.000000 avg.loss:  0.242580 ETA:   0h 0m 0s  0h 0m36s 95.2% words/sec/thread:  734885 lr:  0.004785 avg.loss:  0.246694 ETA:   0h 0m26s\n"
     ]
    }
   ],
   "source": [
    "ftc = FastTextClassifier(\n",
    "    train_data_fn=\"data/amazon5core_medium.txt\",\n",
    "    batch_size=100000,\n",
    "    labels=[1, 3, 5],\n",
    "    kwargs_train_supervised={\n",
    "        \"epoch\": epochs,\n",
    "        \"thread\": 2,\n",
    "        \"lr\": lr,\n",
    "        \"wordNgrams\": ngram,\n",
    "        \"bucket\": 200000,\n",
    "        \"dim\": dim,\n",
    "        \"loss\": \"softmax\",  # possible: 'softmax', 'hs'\n",
    "    },\n",
    ")\n",
    "\n",
    "predictions = cleanlab.count.estimate_cv_predicted_probabilities(\n",
    "    X=np.arange(len(labels)),\n",
    "    labels=labels,\n",
    "    clf=ftc, # model\n",
    "    cv_n_folds=cv_n_folds,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "output_file_name = (\n",
    "    \"data/\"\n",
    "    + \"amazon_pyx_cv__folds_{}__epochs_{}__lr_{}__ngram_{}__dim_{}.npy\".format(\n",
    "        cv_n_folds, epochs, lr, ngram, dim\n",
    "    )\n",
    ")\n",
    "with open(output_file_name, \"wb\") as f:\n",
    "    np.save(f, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model's performance (on 5-CV folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load(\"data/amazon_pyx_cv__folds_5__epochs_5__lr_0.1__ngram_3__dim_100.npy\")\n",
    "labels = np.load(\"data/amazon5core_medium.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8935289646343415"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean labels, try all the possible methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of errors by the method prune_by_class: 61072 | 6.1%\n",
      "Estimated number of errors by the method prune_by_noise_rate: 60555 | 6.1%\n",
      "Estimated number of errors by the method both: 51166 | 5.1%\n",
      "Estimated number of errors by the method confident_learning: 44667 | 4.5%\n",
      "Estimated number of errors by the method predicted_neq_given: 106454 | 10.6%\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "for filter_by in ['prune_by_class', 'prune_by_noise_rate', 'both', 'confident_learning', 'predicted_neq_given']:\n",
    "    label_error_indices = cleanlab.filter.find_label_issues(\n",
    "        labels=labels,\n",
    "        pred_probs=predictions,\n",
    "        filter_by=filter_by,\n",
    "        multi_label=False,\n",
    "        # return_indices_ranked_by='self_confidence', # this only reorders the result, if None then returns boolean mask\n",
    "    )\n",
    "    num_errors = np.sum(label_error_indices)\n",
    "    temp[filter_by] = label_error_indices\n",
    "    print(f'Estimated number of errors by the method {filter_by}: {num_errors} | {100*np.round(num_errors / len(labels), 3)}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "reproducing_amazon_reviews.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
